{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEUwcVBSa-Ge",
        "outputId": "1ea203e6-0a58-4d32-a343-471e82ad1cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/500]  Loss D: 0.2601, Loss G: 5.4431\n",
            "Epoch [2/500]  Loss D: 0.2813, Loss G: 4.2664\n",
            "Epoch [3/500]  Loss D: 0.2888, Loss G: 2.2455\n",
            "Epoch [4/500]  Loss D: 0.2204, Loss G: 3.1708\n",
            "Epoch [5/500]  Loss D: 0.2450, Loss G: 2.2851\n",
            "Epoch [6/500]  Loss D: 0.2261, Loss G: 2.8419\n",
            "Epoch [7/500]  Loss D: 0.2751, Loss G: 2.2979\n",
            "Epoch [8/500]  Loss D: 0.3154, Loss G: 2.2672\n",
            "Epoch [9/500]  Loss D: 0.2365, Loss G: 2.4570\n",
            "Epoch [10/500]  Loss D: 0.2251, Loss G: 2.4040\n",
            "Epoch [11/500]  Loss D: 0.3018, Loss G: 2.8377\n",
            "Epoch [12/500]  Loss D: 0.2424, Loss G: 2.9397\n",
            "Epoch [13/500]  Loss D: 0.2505, Loss G: 4.3046\n",
            "Epoch [14/500]  Loss D: 0.2183, Loss G: 2.5180\n",
            "Epoch [15/500]  Loss D: 0.2391, Loss G: 3.3543\n",
            "Epoch [16/500]  Loss D: 0.2283, Loss G: 2.7990\n",
            "Epoch [17/500]  Loss D: 0.2140, Loss G: 3.7090\n",
            "Epoch [18/500]  Loss D: 0.2140, Loss G: 3.4815\n",
            "Epoch [19/500]  Loss D: 0.2561, Loss G: 2.3322\n",
            "Epoch [20/500]  Loss D: 0.2396, Loss G: 2.7938\n",
            "Epoch [21/500]  Loss D: 0.2925, Loss G: 2.0746\n",
            "Epoch [22/500]  Loss D: 0.3079, Loss G: 1.7329\n",
            "Epoch [23/500]  Loss D: 0.2984, Loss G: 2.3651\n",
            "Epoch [24/500]  Loss D: 0.3230, Loss G: 1.9748\n",
            "Epoch [25/500]  Loss D: 0.3136, Loss G: 1.8833\n",
            "Epoch [26/500]  Loss D: 0.3475, Loss G: 1.9848\n",
            "Epoch [27/500]  Loss D: 0.3569, Loss G: 2.6011\n",
            "Epoch [28/500]  Loss D: 0.3842, Loss G: 1.6922\n",
            "Epoch [29/500]  Loss D: 0.3896, Loss G: 1.4969\n",
            "Epoch [30/500]  Loss D: 0.4052, Loss G: 2.2203\n",
            "Epoch [31/500]  Loss D: 0.3679, Loss G: 1.5183\n",
            "Epoch [32/500]  Loss D: 0.4101, Loss G: 1.3855\n",
            "Epoch [33/500]  Loss D: 0.3800, Loss G: 2.4129\n",
            "Epoch [34/500]  Loss D: 0.3650, Loss G: 1.6750\n",
            "Epoch [35/500]  Loss D: 0.3610, Loss G: 1.7621\n",
            "Epoch [36/500]  Loss D: 0.4034, Loss G: 1.2507\n",
            "Epoch [37/500]  Loss D: 0.3813, Loss G: 1.7892\n",
            "Epoch [38/500]  Loss D: 0.4085, Loss G: 1.6082\n",
            "Epoch [39/500]  Loss D: 0.3196, Loss G: 1.9029\n",
            "Epoch [40/500]  Loss D: 0.3935, Loss G: 1.8750\n",
            "Epoch [41/500]  Loss D: 0.3812, Loss G: 1.7177\n",
            "Epoch [42/500]  Loss D: 0.3376, Loss G: 2.1805\n",
            "Epoch [43/500]  Loss D: 0.3705, Loss G: 2.3824\n",
            "Epoch [44/500]  Loss D: 0.3356, Loss G: 2.2506\n",
            "Epoch [45/500]  Loss D: 0.3388, Loss G: 1.8422\n",
            "Epoch [46/500]  Loss D: 0.4509, Loss G: 2.5826\n",
            "Epoch [47/500]  Loss D: 0.4639, Loss G: 1.4225\n",
            "Epoch [48/500]  Loss D: 0.3613, Loss G: 2.0249\n",
            "Epoch [49/500]  Loss D: 0.4031, Loss G: 2.1892\n",
            "Epoch [50/500]  Loss D: 0.3952, Loss G: 1.7141\n",
            "Epoch [51/500]  Loss D: 0.4933, Loss G: 1.1616\n",
            "Epoch [52/500]  Loss D: 0.3842, Loss G: 1.9772\n",
            "Epoch [53/500]  Loss D: 0.3498, Loss G: 1.9357\n",
            "Epoch [54/500]  Loss D: 0.4099, Loss G: 1.5432\n",
            "Epoch [55/500]  Loss D: 0.4167, Loss G: 1.5923\n",
            "Epoch [56/500]  Loss D: 0.4155, Loss G: 1.6817\n",
            "Epoch [57/500]  Loss D: 0.4948, Loss G: 1.4785\n",
            "Epoch [58/500]  Loss D: 0.5389, Loss G: 1.0403\n",
            "Epoch [59/500]  Loss D: 0.3805, Loss G: 1.7538\n",
            "Epoch [60/500]  Loss D: 0.4827, Loss G: 1.5094\n",
            "Epoch [61/500]  Loss D: 0.4361, Loss G: 1.5843\n",
            "Epoch [62/500]  Loss D: 0.4652, Loss G: 1.4914\n",
            "Epoch [63/500]  Loss D: 0.4647, Loss G: 1.3503\n",
            "Epoch [64/500]  Loss D: 0.4513, Loss G: 1.7868\n",
            "Epoch [65/500]  Loss D: 0.5108, Loss G: 1.7892\n",
            "Epoch [66/500]  Loss D: 0.4821, Loss G: 1.5041\n",
            "Epoch [67/500]  Loss D: 0.4784, Loss G: 1.6487\n",
            "Epoch [68/500]  Loss D: 0.4607, Loss G: 1.4655\n",
            "Epoch [69/500]  Loss D: 0.5172, Loss G: 1.2473\n",
            "Epoch [70/500]  Loss D: 0.5415, Loss G: 1.6542\n",
            "Epoch [71/500]  Loss D: 0.5616, Loss G: 1.4003\n",
            "Epoch [72/500]  Loss D: 0.5143, Loss G: 1.4632\n",
            "Epoch [73/500]  Loss D: 0.5165, Loss G: 1.3866\n",
            "Epoch [74/500]  Loss D: 0.5990, Loss G: 1.2286\n",
            "Epoch [75/500]  Loss D: 0.5465, Loss G: 1.3273\n",
            "Epoch [76/500]  Loss D: 0.5581, Loss G: 1.3110\n",
            "Epoch [77/500]  Loss D: 0.5735, Loss G: 1.0233\n",
            "Epoch [78/500]  Loss D: 0.5383, Loss G: 1.2499\n",
            "Epoch [79/500]  Loss D: 0.5623, Loss G: 0.9887\n",
            "Epoch [80/500]  Loss D: 0.4651, Loss G: 1.1566\n",
            "Epoch [81/500]  Loss D: 0.6097, Loss G: 1.3595\n",
            "Epoch [82/500]  Loss D: 0.5424, Loss G: 1.4035\n",
            "Epoch [83/500]  Loss D: 0.4902, Loss G: 1.5396\n",
            "Epoch [84/500]  Loss D: 0.4962, Loss G: 1.3439\n",
            "Epoch [85/500]  Loss D: 0.6026, Loss G: 1.3591\n",
            "Epoch [86/500]  Loss D: 0.5620, Loss G: 1.3216\n",
            "Epoch [87/500]  Loss D: 0.5572, Loss G: 1.0812\n",
            "Epoch [88/500]  Loss D: 0.5282, Loss G: 1.2255\n",
            "Epoch [89/500]  Loss D: 0.5371, Loss G: 1.2632\n",
            "Epoch [90/500]  Loss D: 0.5105, Loss G: 1.3041\n",
            "Epoch [91/500]  Loss D: 0.5537, Loss G: 1.1364\n",
            "Epoch [92/500]  Loss D: 0.5599, Loss G: 1.2531\n",
            "Epoch [93/500]  Loss D: 0.5521, Loss G: 1.1066\n",
            "Epoch [94/500]  Loss D: 0.5840, Loss G: 1.1073\n",
            "Epoch [95/500]  Loss D: 0.6150, Loss G: 1.1733\n",
            "Epoch [96/500]  Loss D: 0.5926, Loss G: 1.1454\n",
            "Epoch [97/500]  Loss D: 0.6082, Loss G: 1.1629\n",
            "Epoch [98/500]  Loss D: 0.5817, Loss G: 1.0978\n",
            "Epoch [99/500]  Loss D: 0.5182, Loss G: 1.3222\n",
            "Epoch [100/500]  Loss D: 0.5194, Loss G: 1.2176\n",
            "Epoch [101/500]  Loss D: 0.5690, Loss G: 1.1042\n",
            "Epoch [102/500]  Loss D: 0.6049, Loss G: 1.0262\n",
            "Epoch [103/500]  Loss D: 0.5855, Loss G: 1.0813\n",
            "Epoch [104/500]  Loss D: 0.5851, Loss G: 1.0114\n",
            "Epoch [105/500]  Loss D: 0.5589, Loss G: 1.0920\n",
            "Epoch [106/500]  Loss D: 0.6019, Loss G: 0.9855\n",
            "Epoch [107/500]  Loss D: 0.6056, Loss G: 1.0617\n",
            "Epoch [108/500]  Loss D: 0.6480, Loss G: 0.9608\n",
            "Epoch [109/500]  Loss D: 0.6552, Loss G: 0.9235\n",
            "Epoch [110/500]  Loss D: 0.6344, Loss G: 0.9339\n",
            "Epoch [111/500]  Loss D: 0.6569, Loss G: 0.9655\n",
            "Epoch [112/500]  Loss D: 0.6335, Loss G: 1.0243\n",
            "Epoch [113/500]  Loss D: 0.6484, Loss G: 1.0237\n",
            "Epoch [114/500]  Loss D: 0.6582, Loss G: 0.9587\n",
            "Epoch [115/500]  Loss D: 0.6326, Loss G: 1.0076\n",
            "Epoch [116/500]  Loss D: 0.6854, Loss G: 0.8865\n",
            "Epoch [117/500]  Loss D: 0.6419, Loss G: 0.9123\n",
            "Epoch [118/500]  Loss D: 0.6544, Loss G: 0.9543\n",
            "Epoch [119/500]  Loss D: 0.6512, Loss G: 0.9753\n",
            "Epoch [120/500]  Loss D: 0.6522, Loss G: 0.8994\n",
            "Epoch [121/500]  Loss D: 0.6923, Loss G: 0.8892\n",
            "Epoch [122/500]  Loss D: 0.6831, Loss G: 0.8658\n",
            "Epoch [123/500]  Loss D: 0.6912, Loss G: 0.8572\n",
            "Epoch [124/500]  Loss D: 0.6571, Loss G: 0.9221\n",
            "Epoch [125/500]  Loss D: 0.6689, Loss G: 0.9121\n",
            "Epoch [126/500]  Loss D: 0.7300, Loss G: 0.8837\n",
            "Epoch [127/500]  Loss D: 0.6708, Loss G: 0.9300\n",
            "Epoch [128/500]  Loss D: 0.6740, Loss G: 0.9160\n",
            "Epoch [129/500]  Loss D: 0.6811, Loss G: 0.8689\n",
            "Epoch [130/500]  Loss D: 0.7082, Loss G: 0.8533\n",
            "Epoch [131/500]  Loss D: 0.6752, Loss G: 0.8980\n",
            "Epoch [132/500]  Loss D: 0.6957, Loss G: 0.8182\n",
            "Epoch [133/500]  Loss D: 0.7139, Loss G: 0.8345\n",
            "Epoch [134/500]  Loss D: 0.6499, Loss G: 0.8795\n",
            "Epoch [135/500]  Loss D: 0.6689, Loss G: 0.8501\n",
            "Epoch [136/500]  Loss D: 0.6789, Loss G: 0.8649\n",
            "Epoch [137/500]  Loss D: 0.6793, Loss G: 0.8542\n",
            "Epoch [138/500]  Loss D: 0.6786, Loss G: 0.8879\n",
            "Epoch [139/500]  Loss D: 0.7130, Loss G: 0.8281\n",
            "Epoch [140/500]  Loss D: 0.6511, Loss G: 0.8878\n",
            "Epoch [141/500]  Loss D: 0.6814, Loss G: 0.8258\n",
            "Epoch [142/500]  Loss D: 0.6733, Loss G: 0.8485\n",
            "Epoch [143/500]  Loss D: 0.7121, Loss G: 0.7989\n",
            "Epoch [144/500]  Loss D: 0.6823, Loss G: 0.8541\n",
            "Epoch [145/500]  Loss D: 0.6621, Loss G: 0.8227\n",
            "Epoch [146/500]  Loss D: 0.6974, Loss G: 0.8288\n",
            "Epoch [147/500]  Loss D: 0.6729, Loss G: 0.8302\n",
            "Epoch [148/500]  Loss D: 0.6721, Loss G: 0.8416\n",
            "Epoch [149/500]  Loss D: 0.6908, Loss G: 0.8503\n",
            "Epoch [150/500]  Loss D: 0.6904, Loss G: 0.8419\n",
            "Epoch [151/500]  Loss D: 0.6731, Loss G: 0.8497\n",
            "Epoch [152/500]  Loss D: 0.6792, Loss G: 0.8335\n",
            "Epoch [153/500]  Loss D: 0.6918, Loss G: 0.8197\n",
            "Epoch [154/500]  Loss D: 0.6903, Loss G: 0.8349\n",
            "Epoch [155/500]  Loss D: 0.6760, Loss G: 0.8046\n",
            "Epoch [156/500]  Loss D: 0.6779, Loss G: 0.8286\n",
            "Epoch [157/500]  Loss D: 0.6815, Loss G: 0.8497\n",
            "Epoch [158/500]  Loss D: 0.6765, Loss G: 0.8710\n",
            "Epoch [159/500]  Loss D: 0.6721, Loss G: 0.8491\n",
            "Epoch [160/500]  Loss D: 0.6712, Loss G: 0.8943\n",
            "Epoch [161/500]  Loss D: 0.6904, Loss G: 0.8328\n",
            "Epoch [162/500]  Loss D: 0.6752, Loss G: 0.8730\n",
            "Epoch [163/500]  Loss D: 0.6799, Loss G: 0.8459\n",
            "Epoch [164/500]  Loss D: 0.6779, Loss G: 0.8298\n",
            "Epoch [165/500]  Loss D: 0.6833, Loss G: 0.8512\n",
            "Epoch [166/500]  Loss D: 0.7007, Loss G: 0.7900\n",
            "Epoch [167/500]  Loss D: 0.6782, Loss G: 0.8229\n",
            "Epoch [168/500]  Loss D: 0.6806, Loss G: 0.8281\n",
            "Epoch [169/500]  Loss D: 0.6911, Loss G: 0.8586\n",
            "Epoch [170/500]  Loss D: 0.6793, Loss G: 0.8512\n",
            "Epoch [171/500]  Loss D: 0.6800, Loss G: 0.8046\n",
            "Epoch [172/500]  Loss D: 0.7246, Loss G: 0.8168\n",
            "Epoch [173/500]  Loss D: 0.6762, Loss G: 0.8516\n",
            "Epoch [174/500]  Loss D: 0.6649, Loss G: 0.8276\n",
            "Epoch [175/500]  Loss D: 0.6861, Loss G: 0.8422\n",
            "Epoch [176/500]  Loss D: 0.7043, Loss G: 0.7948\n",
            "Epoch [177/500]  Loss D: 0.6834, Loss G: 0.8065\n",
            "Epoch [178/500]  Loss D: 0.7029, Loss G: 0.8216\n",
            "Epoch [179/500]  Loss D: 0.7112, Loss G: 0.7925\n",
            "Epoch [180/500]  Loss D: 0.6785, Loss G: 0.7917\n",
            "Epoch [181/500]  Loss D: 0.6839, Loss G: 0.8069\n",
            "Epoch [182/500]  Loss D: 0.6915, Loss G: 0.8077\n",
            "Epoch [183/500]  Loss D: 0.6697, Loss G: 0.8631\n",
            "Epoch [184/500]  Loss D: 0.6939, Loss G: 0.8162\n",
            "Epoch [185/500]  Loss D: 0.6971, Loss G: 0.8304\n",
            "Epoch [186/500]  Loss D: 0.7041, Loss G: 0.7755\n",
            "Epoch [187/500]  Loss D: 0.7024, Loss G: 0.8050\n",
            "Epoch [188/500]  Loss D: 0.6921, Loss G: 0.8346\n",
            "Epoch [189/500]  Loss D: 0.6839, Loss G: 0.8195\n",
            "Epoch [190/500]  Loss D: 0.6873, Loss G: 0.8170\n",
            "Epoch [191/500]  Loss D: 0.6830, Loss G: 0.8332\n",
            "Epoch [192/500]  Loss D: 0.6592, Loss G: 0.8409\n",
            "Epoch [193/500]  Loss D: 0.6764, Loss G: 0.8582\n",
            "Epoch [194/500]  Loss D: 0.6855, Loss G: 0.8470\n",
            "Epoch [195/500]  Loss D: 0.6796, Loss G: 0.8080\n",
            "Epoch [196/500]  Loss D: 0.7066, Loss G: 0.7893\n",
            "Epoch [197/500]  Loss D: 0.7178, Loss G: 0.7392\n",
            "Epoch [198/500]  Loss D: 0.6705, Loss G: 0.8244\n",
            "Epoch [199/500]  Loss D: 0.6908, Loss G: 0.8409\n",
            "Epoch [200/500]  Loss D: 0.6648, Loss G: 0.8217\n",
            "Epoch [201/500]  Loss D: 0.6941, Loss G: 0.8086\n",
            "Epoch [202/500]  Loss D: 0.6557, Loss G: 0.8485\n",
            "Epoch [203/500]  Loss D: 0.6609, Loss G: 0.8140\n",
            "Epoch [204/500]  Loss D: 0.6843, Loss G: 0.7973\n",
            "Epoch [205/500]  Loss D: 0.6936, Loss G: 0.8275\n",
            "Epoch [206/500]  Loss D: 0.7010, Loss G: 0.7992\n",
            "Epoch [207/500]  Loss D: 0.6713, Loss G: 0.8353\n",
            "Epoch [208/500]  Loss D: 0.6892, Loss G: 0.8094\n",
            "Epoch [209/500]  Loss D: 0.6852, Loss G: 0.7908\n",
            "Epoch [210/500]  Loss D: 0.6762, Loss G: 0.8409\n",
            "Epoch [211/500]  Loss D: 0.6934, Loss G: 0.8027\n",
            "Epoch [212/500]  Loss D: 0.6912, Loss G: 0.8186\n",
            "Epoch [213/500]  Loss D: 0.6903, Loss G: 0.8324\n",
            "Epoch [214/500]  Loss D: 0.7171, Loss G: 0.7967\n",
            "Epoch [215/500]  Loss D: 0.6839, Loss G: 0.8233\n",
            "Epoch [216/500]  Loss D: 0.7080, Loss G: 0.8010\n",
            "Epoch [217/500]  Loss D: 0.6865, Loss G: 0.7914\n",
            "Epoch [218/500]  Loss D: 0.7056, Loss G: 0.8007\n",
            "Epoch [219/500]  Loss D: 0.6575, Loss G: 0.8343\n",
            "Epoch [220/500]  Loss D: 0.7124, Loss G: 0.7868\n",
            "Epoch [221/500]  Loss D: 0.7022, Loss G: 0.8274\n",
            "Epoch [222/500]  Loss D: 0.7176, Loss G: 0.8029\n",
            "Epoch [223/500]  Loss D: 0.6775, Loss G: 0.8244\n",
            "Epoch [224/500]  Loss D: 0.7016, Loss G: 0.8048\n",
            "Epoch [225/500]  Loss D: 0.6822, Loss G: 0.8278\n",
            "Epoch [226/500]  Loss D: 0.6567, Loss G: 0.8584\n",
            "Epoch [227/500]  Loss D: 0.6910, Loss G: 0.8013\n",
            "Epoch [228/500]  Loss D: 0.6628, Loss G: 0.8510\n",
            "Epoch [229/500]  Loss D: 0.6871, Loss G: 0.7821\n",
            "Epoch [230/500]  Loss D: 0.6821, Loss G: 0.8136\n",
            "Epoch [231/500]  Loss D: 0.6837, Loss G: 0.8099\n",
            "Epoch [232/500]  Loss D: 0.6801, Loss G: 0.8131\n",
            "Epoch [233/500]  Loss D: 0.6731, Loss G: 0.8391\n",
            "Epoch [234/500]  Loss D: 0.6823, Loss G: 0.8523\n",
            "Epoch [235/500]  Loss D: 0.6948, Loss G: 0.7843\n",
            "Epoch [236/500]  Loss D: 0.6945, Loss G: 0.8053\n",
            "Epoch [237/500]  Loss D: 0.7057, Loss G: 0.7775\n",
            "Epoch [238/500]  Loss D: 0.6851, Loss G: 0.8153\n",
            "Epoch [239/500]  Loss D: 0.7201, Loss G: 0.7853\n",
            "Epoch [240/500]  Loss D: 0.6861, Loss G: 0.8291\n",
            "Epoch [241/500]  Loss D: 0.6888, Loss G: 0.8272\n",
            "Epoch [242/500]  Loss D: 0.7044, Loss G: 0.7865\n",
            "Epoch [243/500]  Loss D: 0.6977, Loss G: 0.7912\n",
            "Epoch [244/500]  Loss D: 0.6807, Loss G: 0.8142\n",
            "Epoch [245/500]  Loss D: 0.6676, Loss G: 0.8485\n",
            "Epoch [246/500]  Loss D: 0.6922, Loss G: 0.8234\n",
            "Epoch [247/500]  Loss D: 0.6727, Loss G: 0.8251\n",
            "Epoch [248/500]  Loss D: 0.6940, Loss G: 0.8074\n",
            "Epoch [249/500]  Loss D: 0.6719, Loss G: 0.8342\n",
            "Epoch [250/500]  Loss D: 0.6887, Loss G: 0.8044\n",
            "Epoch [251/500]  Loss D: 0.6994, Loss G: 0.7996\n",
            "Epoch [252/500]  Loss D: 0.6902, Loss G: 0.8233\n",
            "Epoch [253/500]  Loss D: 0.6967, Loss G: 0.8102\n",
            "Epoch [254/500]  Loss D: 0.6922, Loss G: 0.7739\n",
            "Epoch [255/500]  Loss D: 0.7067, Loss G: 0.7806\n",
            "Epoch [256/500]  Loss D: 0.6881, Loss G: 0.7992\n",
            "Epoch [257/500]  Loss D: 0.6632, Loss G: 0.8371\n",
            "Epoch [258/500]  Loss D: 0.6701, Loss G: 0.8413\n",
            "Epoch [259/500]  Loss D: 0.6923, Loss G: 0.8047\n",
            "Epoch [260/500]  Loss D: 0.7038, Loss G: 0.8128\n",
            "Epoch [261/500]  Loss D: 0.7016, Loss G: 0.8051\n",
            "Epoch [262/500]  Loss D: 0.6811, Loss G: 0.8221\n",
            "Epoch [263/500]  Loss D: 0.6991, Loss G: 0.8085\n",
            "Epoch [264/500]  Loss D: 0.7185, Loss G: 0.8028\n",
            "Epoch [265/500]  Loss D: 0.6743, Loss G: 0.8379\n",
            "Epoch [266/500]  Loss D: 0.6889, Loss G: 0.8042\n",
            "Epoch [267/500]  Loss D: 0.6990, Loss G: 0.8112\n",
            "Epoch [268/500]  Loss D: 0.6487, Loss G: 0.8400\n",
            "Epoch [269/500]  Loss D: 0.6816, Loss G: 0.8096\n",
            "Epoch [270/500]  Loss D: 0.6932, Loss G: 0.8099\n",
            "Epoch [271/500]  Loss D: 0.6810, Loss G: 0.8356\n",
            "Epoch [272/500]  Loss D: 0.6867, Loss G: 0.8043\n",
            "Epoch [273/500]  Loss D: 0.7063, Loss G: 0.8040\n",
            "Epoch [274/500]  Loss D: 0.6877, Loss G: 0.8001\n",
            "Epoch [275/500]  Loss D: 0.6911, Loss G: 0.8112\n",
            "Epoch [276/500]  Loss D: 0.7170, Loss G: 0.8139\n",
            "Epoch [277/500]  Loss D: 0.6994, Loss G: 0.7705\n",
            "Epoch [278/500]  Loss D: 0.6912, Loss G: 0.8135\n",
            "Epoch [279/500]  Loss D: 0.6799, Loss G: 0.8095\n",
            "Epoch [280/500]  Loss D: 0.6860, Loss G: 0.8051\n",
            "Epoch [281/500]  Loss D: 0.6730, Loss G: 0.8265\n",
            "Epoch [282/500]  Loss D: 0.6769, Loss G: 0.8081\n",
            "Epoch [283/500]  Loss D: 0.6845, Loss G: 0.8110\n",
            "Epoch [284/500]  Loss D: 0.6916, Loss G: 0.7914\n",
            "Epoch [285/500]  Loss D: 0.6934, Loss G: 0.8016\n",
            "Epoch [286/500]  Loss D: 0.6926, Loss G: 0.8358\n",
            "Epoch [287/500]  Loss D: 0.7036, Loss G: 0.7910\n",
            "Epoch [288/500]  Loss D: 0.6862, Loss G: 0.8096\n",
            "Epoch [289/500]  Loss D: 0.6899, Loss G: 0.7729\n",
            "Epoch [290/500]  Loss D: 0.6704, Loss G: 0.8148\n",
            "Epoch [291/500]  Loss D: 0.6704, Loss G: 0.8157\n",
            "Epoch [292/500]  Loss D: 0.6904, Loss G: 0.8051\n",
            "Epoch [293/500]  Loss D: 0.6949, Loss G: 0.8209\n",
            "Epoch [294/500]  Loss D: 0.6700, Loss G: 0.8365\n",
            "Epoch [295/500]  Loss D: 0.6920, Loss G: 0.8129\n",
            "Epoch [296/500]  Loss D: 0.6746, Loss G: 0.8200\n",
            "Epoch [297/500]  Loss D: 0.7095, Loss G: 0.8123\n",
            "Epoch [298/500]  Loss D: 0.6728, Loss G: 0.8200\n",
            "Epoch [299/500]  Loss D: 0.6742, Loss G: 0.8150\n",
            "Epoch [300/500]  Loss D: 0.6788, Loss G: 0.8065\n",
            "Epoch [301/500]  Loss D: 0.6830, Loss G: 0.8194\n",
            "Epoch [302/500]  Loss D: 0.7108, Loss G: 0.7877\n",
            "Epoch [303/500]  Loss D: 0.6790, Loss G: 0.8147\n",
            "Epoch [304/500]  Loss D: 0.6932, Loss G: 0.7810\n",
            "Epoch [305/500]  Loss D: 0.6989, Loss G: 0.7727\n",
            "Epoch [306/500]  Loss D: 0.7060, Loss G: 0.8031\n",
            "Epoch [307/500]  Loss D: 0.6835, Loss G: 0.8280\n",
            "Epoch [308/500]  Loss D: 0.6805, Loss G: 0.7979\n",
            "Epoch [309/500]  Loss D: 0.6630, Loss G: 0.8495\n",
            "Epoch [310/500]  Loss D: 0.6782, Loss G: 0.8354\n",
            "Epoch [311/500]  Loss D: 0.7047, Loss G: 0.8168\n",
            "Epoch [312/500]  Loss D: 0.6722, Loss G: 0.8439\n",
            "Epoch [313/500]  Loss D: 0.6984, Loss G: 0.8121\n",
            "Epoch [314/500]  Loss D: 0.6765, Loss G: 0.8228\n",
            "Epoch [315/500]  Loss D: 0.6645, Loss G: 0.8388\n",
            "Epoch [316/500]  Loss D: 0.6961, Loss G: 0.7942\n",
            "Epoch [317/500]  Loss D: 0.6936, Loss G: 0.7841\n",
            "Epoch [318/500]  Loss D: 0.6913, Loss G: 0.8186\n",
            "Epoch [319/500]  Loss D: 0.7008, Loss G: 0.8137\n",
            "Epoch [320/500]  Loss D: 0.6834, Loss G: 0.8161\n",
            "Epoch [321/500]  Loss D: 0.6628, Loss G: 0.8105\n",
            "Epoch [322/500]  Loss D: 0.6921, Loss G: 0.8112\n",
            "Epoch [323/500]  Loss D: 0.6813, Loss G: 0.8204\n",
            "Epoch [324/500]  Loss D: 0.7017, Loss G: 0.8102\n",
            "Epoch [325/500]  Loss D: 0.6680, Loss G: 0.8086\n",
            "Epoch [326/500]  Loss D: 0.6554, Loss G: 0.8203\n",
            "Epoch [327/500]  Loss D: 0.6823, Loss G: 0.8126\n",
            "Epoch [328/500]  Loss D: 0.6809, Loss G: 0.8315\n",
            "Epoch [329/500]  Loss D: 0.6833, Loss G: 0.8046\n",
            "Epoch [330/500]  Loss D: 0.6986, Loss G: 0.8166\n",
            "Epoch [331/500]  Loss D: 0.6945, Loss G: 0.7919\n",
            "Epoch [332/500]  Loss D: 0.6984, Loss G: 0.8051\n",
            "Epoch [333/500]  Loss D: 0.6781, Loss G: 0.8008\n",
            "Epoch [334/500]  Loss D: 0.6897, Loss G: 0.7761\n",
            "Epoch [335/500]  Loss D: 0.6645, Loss G: 0.8144\n",
            "Epoch [336/500]  Loss D: 0.7163, Loss G: 0.7704\n",
            "Epoch [337/500]  Loss D: 0.6810, Loss G: 0.8039\n",
            "Epoch [338/500]  Loss D: 0.7204, Loss G: 0.7731\n",
            "Epoch [339/500]  Loss D: 0.6917, Loss G: 0.7896\n",
            "Epoch [340/500]  Loss D: 0.6877, Loss G: 0.8188\n",
            "Epoch [341/500]  Loss D: 0.7007, Loss G: 0.7912\n",
            "Epoch [342/500]  Loss D: 0.6952, Loss G: 0.8144\n",
            "Epoch [343/500]  Loss D: 0.6903, Loss G: 0.7956\n",
            "Epoch [344/500]  Loss D: 0.7027, Loss G: 0.7945\n",
            "Epoch [345/500]  Loss D: 0.7062, Loss G: 0.7858\n",
            "Epoch [346/500]  Loss D: 0.6899, Loss G: 0.8082\n",
            "Epoch [347/500]  Loss D: 0.6964, Loss G: 0.7972\n",
            "Epoch [348/500]  Loss D: 0.6780, Loss G: 0.8144\n",
            "Epoch [349/500]  Loss D: 0.6955, Loss G: 0.7949\n",
            "Epoch [350/500]  Loss D: 0.6972, Loss G: 0.8146\n",
            "Epoch [351/500]  Loss D: 0.6983, Loss G: 0.7993\n",
            "Epoch [352/500]  Loss D: 0.6939, Loss G: 0.7970\n",
            "Epoch [353/500]  Loss D: 0.6938, Loss G: 0.8201\n",
            "Epoch [354/500]  Loss D: 0.6861, Loss G: 0.8053\n",
            "Epoch [355/500]  Loss D: 0.6636, Loss G: 0.8304\n",
            "Epoch [356/500]  Loss D: 0.6891, Loss G: 0.8008\n",
            "Epoch [357/500]  Loss D: 0.6922, Loss G: 0.8000\n",
            "Epoch [358/500]  Loss D: 0.6946, Loss G: 0.8068\n",
            "Epoch [359/500]  Loss D: 0.6754, Loss G: 0.8000\n",
            "Epoch [360/500]  Loss D: 0.7102, Loss G: 0.7691\n",
            "Epoch [361/500]  Loss D: 0.6875, Loss G: 0.8220\n",
            "Epoch [362/500]  Loss D: 0.6585, Loss G: 0.8461\n",
            "Epoch [363/500]  Loss D: 0.6884, Loss G: 0.7832\n",
            "Epoch [364/500]  Loss D: 0.6729, Loss G: 0.8246\n",
            "Epoch [365/500]  Loss D: 0.6807, Loss G: 0.7921\n",
            "Epoch [366/500]  Loss D: 0.6952, Loss G: 0.8047\n",
            "Epoch [367/500]  Loss D: 0.6858, Loss G: 0.8066\n",
            "Epoch [368/500]  Loss D: 0.6762, Loss G: 0.8237\n",
            "Epoch [369/500]  Loss D: 0.7008, Loss G: 0.7978\n",
            "Epoch [370/500]  Loss D: 0.7037, Loss G: 0.8040\n",
            "Epoch [371/500]  Loss D: 0.6876, Loss G: 0.8080\n",
            "Epoch [372/500]  Loss D: 0.7014, Loss G: 0.7999\n",
            "Epoch [373/500]  Loss D: 0.6658, Loss G: 0.8191\n",
            "Epoch [374/500]  Loss D: 0.7090, Loss G: 0.7948\n",
            "Epoch [375/500]  Loss D: 0.6822, Loss G: 0.8243\n",
            "Epoch [376/500]  Loss D: 0.6597, Loss G: 0.8259\n",
            "Epoch [377/500]  Loss D: 0.6881, Loss G: 0.8064\n",
            "Epoch [378/500]  Loss D: 0.6879, Loss G: 0.8231\n",
            "Epoch [379/500]  Loss D: 0.6882, Loss G: 0.8267\n",
            "Epoch [380/500]  Loss D: 0.6753, Loss G: 0.8206\n",
            "Epoch [381/500]  Loss D: 0.6840, Loss G: 0.8083\n",
            "Epoch [382/500]  Loss D: 0.6903, Loss G: 0.8194\n",
            "Epoch [383/500]  Loss D: 0.6812, Loss G: 0.7984\n",
            "Epoch [384/500]  Loss D: 0.6821, Loss G: 0.8177\n",
            "Epoch [385/500]  Loss D: 0.6828, Loss G: 0.8194\n",
            "Epoch [386/500]  Loss D: 0.6924, Loss G: 0.7953\n",
            "Epoch [387/500]  Loss D: 0.6845, Loss G: 0.8299\n",
            "Epoch [388/500]  Loss D: 0.6882, Loss G: 0.7870\n",
            "Epoch [389/500]  Loss D: 0.6838, Loss G: 0.8181\n",
            "Epoch [390/500]  Loss D: 0.6966, Loss G: 0.8047\n",
            "Epoch [391/500]  Loss D: 0.6815, Loss G: 0.8145\n",
            "Epoch [392/500]  Loss D: 0.6933, Loss G: 0.7999\n",
            "Epoch [393/500]  Loss D: 0.6730, Loss G: 0.8044\n",
            "Epoch [394/500]  Loss D: 0.6761, Loss G: 0.8211\n",
            "Epoch [395/500]  Loss D: 0.6843, Loss G: 0.8146\n",
            "Epoch [396/500]  Loss D: 0.6946, Loss G: 0.7814\n",
            "Epoch [397/500]  Loss D: 0.6921, Loss G: 0.8036\n",
            "Epoch [398/500]  Loss D: 0.7221, Loss G: 0.7917\n",
            "Epoch [399/500]  Loss D: 0.6954, Loss G: 0.7838\n",
            "Epoch [400/500]  Loss D: 0.6649, Loss G: 0.8053\n",
            "Epoch [401/500]  Loss D: 0.6876, Loss G: 0.7919\n",
            "Epoch [402/500]  Loss D: 0.6904, Loss G: 0.7852\n",
            "Epoch [403/500]  Loss D: 0.6839, Loss G: 0.7944\n",
            "Epoch [404/500]  Loss D: 0.7165, Loss G: 0.7797\n",
            "Epoch [405/500]  Loss D: 0.6977, Loss G: 0.7942\n",
            "Epoch [406/500]  Loss D: 0.6967, Loss G: 0.7952\n",
            "Epoch [407/500]  Loss D: 0.6971, Loss G: 0.7854\n",
            "Epoch [408/500]  Loss D: 0.6982, Loss G: 0.7868\n",
            "Epoch [409/500]  Loss D: 0.6887, Loss G: 0.8049\n",
            "Epoch [410/500]  Loss D: 0.6863, Loss G: 0.7999\n",
            "Epoch [411/500]  Loss D: 0.6928, Loss G: 0.7696\n",
            "Epoch [412/500]  Loss D: 0.6805, Loss G: 0.8215\n",
            "Epoch [413/500]  Loss D: 0.6877, Loss G: 0.8046\n",
            "Epoch [414/500]  Loss D: 0.6862, Loss G: 0.8154\n",
            "Epoch [415/500]  Loss D: 0.6789, Loss G: 0.8183\n",
            "Epoch [416/500]  Loss D: 0.7095, Loss G: 0.7959\n",
            "Epoch [417/500]  Loss D: 0.6848, Loss G: 0.8193\n",
            "Epoch [418/500]  Loss D: 0.6861, Loss G: 0.7976\n",
            "Epoch [419/500]  Loss D: 0.6943, Loss G: 0.7760\n",
            "Epoch [420/500]  Loss D: 0.7052, Loss G: 0.8255\n",
            "Epoch [421/500]  Loss D: 0.6823, Loss G: 0.7785\n",
            "Epoch [422/500]  Loss D: 0.6919, Loss G: 0.8091\n",
            "Epoch [423/500]  Loss D: 0.6994, Loss G: 0.8013\n",
            "Epoch [424/500]  Loss D: 0.7086, Loss G: 0.7987\n",
            "Epoch [425/500]  Loss D: 0.6857, Loss G: 0.8168\n",
            "Epoch [426/500]  Loss D: 0.7065, Loss G: 0.7876\n",
            "Epoch [427/500]  Loss D: 0.7116, Loss G: 0.8098\n",
            "Epoch [428/500]  Loss D: 0.7017, Loss G: 0.8129\n",
            "Epoch [429/500]  Loss D: 0.6842, Loss G: 0.7989\n",
            "Epoch [430/500]  Loss D: 0.7093, Loss G: 0.7919\n",
            "Epoch [431/500]  Loss D: 0.6842, Loss G: 0.8014\n",
            "Epoch [432/500]  Loss D: 0.6728, Loss G: 0.8372\n",
            "Epoch [433/500]  Loss D: 0.6892, Loss G: 0.8035\n",
            "Epoch [434/500]  Loss D: 0.6769, Loss G: 0.8286\n",
            "Epoch [435/500]  Loss D: 0.6921, Loss G: 0.8087\n",
            "Epoch [436/500]  Loss D: 0.6842, Loss G: 0.7958\n",
            "Epoch [437/500]  Loss D: 0.6968, Loss G: 0.8047\n",
            "Epoch [438/500]  Loss D: 0.6991, Loss G: 0.8036\n",
            "Epoch [439/500]  Loss D: 0.6983, Loss G: 0.8026\n",
            "Epoch [440/500]  Loss D: 0.6857, Loss G: 0.7967\n",
            "Epoch [441/500]  Loss D: 0.6726, Loss G: 0.7851\n",
            "Epoch [442/500]  Loss D: 0.7098, Loss G: 0.8066\n",
            "Epoch [443/500]  Loss D: 0.7039, Loss G: 0.7949\n",
            "Epoch [444/500]  Loss D: 0.7081, Loss G: 0.7921\n",
            "Epoch [445/500]  Loss D: 0.6854, Loss G: 0.8331\n",
            "Epoch [446/500]  Loss D: 0.6750, Loss G: 0.8356\n",
            "Epoch [447/500]  Loss D: 0.7015, Loss G: 0.7628\n",
            "Epoch [448/500]  Loss D: 0.7180, Loss G: 0.7878\n",
            "Epoch [449/500]  Loss D: 0.6772, Loss G: 0.8112\n",
            "Epoch [450/500]  Loss D: 0.7032, Loss G: 0.8154\n",
            "Epoch [451/500]  Loss D: 0.6840, Loss G: 0.8007\n",
            "Epoch [452/500]  Loss D: 0.6886, Loss G: 0.8049\n",
            "Epoch [453/500]  Loss D: 0.7046, Loss G: 0.7931\n",
            "Epoch [454/500]  Loss D: 0.6820, Loss G: 0.8228\n",
            "Epoch [455/500]  Loss D: 0.6704, Loss G: 0.8263\n",
            "Epoch [456/500]  Loss D: 0.6832, Loss G: 0.8283\n",
            "Epoch [457/500]  Loss D: 0.6991, Loss G: 0.8005\n",
            "Epoch [458/500]  Loss D: 0.6682, Loss G: 0.8157\n",
            "Epoch [459/500]  Loss D: 0.6856, Loss G: 0.8082\n",
            "Epoch [460/500]  Loss D: 0.6674, Loss G: 0.8183\n",
            "Epoch [461/500]  Loss D: 0.6843, Loss G: 0.8094\n",
            "Epoch [462/500]  Loss D: 0.6843, Loss G: 0.8192\n",
            "Epoch [463/500]  Loss D: 0.6758, Loss G: 0.8166\n",
            "Epoch [464/500]  Loss D: 0.7021, Loss G: 0.7994\n",
            "Epoch [465/500]  Loss D: 0.7024, Loss G: 0.8182\n",
            "Epoch [466/500]  Loss D: 0.6890, Loss G: 0.8088\n",
            "Epoch [467/500]  Loss D: 0.6787, Loss G: 0.7969\n",
            "Epoch [468/500]  Loss D: 0.6883, Loss G: 0.8063\n",
            "Epoch [469/500]  Loss D: 0.6808, Loss G: 0.7852\n",
            "Epoch [470/500]  Loss D: 0.7054, Loss G: 0.7840\n",
            "Epoch [471/500]  Loss D: 0.6962, Loss G: 0.7946\n",
            "Epoch [472/500]  Loss D: 0.6871, Loss G: 0.8099\n",
            "Epoch [473/500]  Loss D: 0.6866, Loss G: 0.8185\n",
            "Epoch [474/500]  Loss D: 0.6866, Loss G: 0.8044\n",
            "Epoch [475/500]  Loss D: 0.6857, Loss G: 0.8073\n",
            "Epoch [476/500]  Loss D: 0.6804, Loss G: 0.8150\n",
            "Epoch [477/500]  Loss D: 0.6921, Loss G: 0.7726\n",
            "Epoch [478/500]  Loss D: 0.6771, Loss G: 0.7944\n",
            "Epoch [479/500]  Loss D: 0.7011, Loss G: 0.7827\n",
            "Epoch [480/500]  Loss D: 0.6783, Loss G: 0.8317\n",
            "Epoch [481/500]  Loss D: 0.7123, Loss G: 0.7632\n",
            "Epoch [482/500]  Loss D: 0.6971, Loss G: 0.7945\n",
            "Epoch [483/500]  Loss D: 0.6975, Loss G: 0.7898\n",
            "Epoch [484/500]  Loss D: 0.6878, Loss G: 0.8396\n",
            "Epoch [485/500]  Loss D: 0.6972, Loss G: 0.8063\n",
            "Epoch [486/500]  Loss D: 0.6880, Loss G: 0.7914\n",
            "Epoch [487/500]  Loss D: 0.6957, Loss G: 0.8037\n",
            "Epoch [488/500]  Loss D: 0.6807, Loss G: 0.7917\n",
            "Epoch [489/500]  Loss D: 0.6709, Loss G: 0.8073\n",
            "Epoch [490/500]  Loss D: 0.6754, Loss G: 0.7917\n",
            "Epoch [491/500]  Loss D: 0.6960, Loss G: 0.8069\n",
            "Epoch [492/500]  Loss D: 0.6945, Loss G: 0.8099\n",
            "Epoch [493/500]  Loss D: 0.6903, Loss G: 0.8030\n",
            "Epoch [494/500]  Loss D: 0.6684, Loss G: 0.8302\n",
            "Epoch [495/500]  Loss D: 0.6763, Loss G: 0.8279\n",
            "Epoch [496/500]  Loss D: 0.6736, Loss G: 0.8244\n",
            "Epoch [497/500]  Loss D: 0.6702, Loss G: 0.8014\n",
            "Epoch [498/500]  Loss D: 0.7003, Loss G: 0.8113\n",
            "Epoch [499/500]  Loss D: 0.6938, Loss G: 0.8183\n",
            "Epoch [500/500]  Loss D: 0.6950, Loss G: 0.8022\n",
            "Training completed ✅\n"
          ]
        }
      ],
      "source": [
        "# ✅ Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "# ✅ Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ Set Paths\n",
        "save_dir = \"/content/drive/MyDrive/cgan_cifar10\"  # Your Google Drive folder\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "os.makedirs(f\"{save_dir}/generated_samples\", exist_ok=True)\n",
        "os.makedirs(f\"{save_dir}/checkpoints\", exist_ok=True)\n",
        "\n",
        "# ✅ Hyperparameters\n",
        "num_epochs = 500\n",
        "batch_size = 128\n",
        "learning_rate = 2e-4\n",
        "noise_dim = 100\n",
        "num_classes = 10\n",
        "img_size = 32\n",
        "channels = 3\n",
        "\n",
        "# ✅ Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # (-1 to 1)\n",
        "])\n",
        "\n",
        "# ✅ Dataset and Loader\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ✅ Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, num_classes):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim + num_classes, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, channels * img_size * img_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        labels = self.label_emb(labels)\n",
        "        x = torch.cat([noise, labels], dim=1)\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), channels, img_size, img_size)\n",
        "        return x\n",
        "\n",
        "# ✅ Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(channels * img_size * img_size + num_classes, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        labels = self.label_emb(labels)\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        d_in = torch.cat([img_flat, labels], dim=1)\n",
        "        validity = self.model(d_in)\n",
        "        return validity\n",
        "\n",
        "# ✅ Initialize models\n",
        "generator = Generator(noise_dim, num_classes).to(device)\n",
        "discriminator = Discriminator(num_classes).to(device)\n",
        "\n",
        "# ✅ Loss and optimizers\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "\n",
        "# ✅ Learning rate schedulers\n",
        "scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=100, gamma=0.1)\n",
        "scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=100, gamma=0.1)\n",
        "\n",
        "# ✅ Fixed noise for samples\n",
        "fixed_noise = torch.randn(25, noise_dim, device=device)\n",
        "fixed_labels = torch.randint(0, num_classes, (25,), device=device)\n",
        "\n",
        "# ✅ Training Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        batch_size_i = imgs.size(0)\n",
        "        real_imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # ✅ Label smoothing\n",
        "        valid = torch.full((batch_size_i, 1), 0.9, device=device)  # Real labels smoothed to 0.9\n",
        "        fake = torch.zeros(batch_size_i, 1, device=device)         # Fake labels 0.0\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        noise = torch.randn(batch_size_i, noise_dim, device=device)\n",
        "        gen_labels = torch.randint(0, num_classes, (batch_size_i,), device=device)\n",
        "        gen_imgs = generator(noise, gen_labels)\n",
        "\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    # ✅ Save generated images\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(fixed_noise, fixed_labels)\n",
        "        gen_imgs = (gen_imgs + 1) / 2.0  # De-normalize\n",
        "        grid = make_grid(gen_imgs, nrow=5)\n",
        "        save_image(grid, f\"{save_dir}/generated_samples/epoch_{epoch}.png\")\n",
        "\n",
        "    # ✅ Save model checkpoints\n",
        "    if epoch % 50 == 0 or epoch == num_epochs:\n",
        "        torch.save(generator.state_dict(), f\"{save_dir}/checkpoints/generator_epoch_{epoch}.pth\")\n",
        "        torch.save(discriminator.state_dict(), f\"{save_dir}/checkpoints/discriminator_epoch_{epoch}.pth\")\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}]  Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "print(\"Training completed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(generator.state_dict(), \"cifar_cgan_generator.pth\")\n",
        "print(\"Saved Conditional CIFAR-10 Generator!\")"
      ],
      "metadata": {
        "id": "asnmC1agUkAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}